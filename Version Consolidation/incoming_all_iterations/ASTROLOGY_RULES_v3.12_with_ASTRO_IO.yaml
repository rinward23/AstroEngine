# ===============================
# Consolidated Astrology I/O Ruleset
# ===============================

modules:
  # -------------------------------
  # CORE TAGS (GLOBAL, PROFILE-AGNOSTIC)
  # -------------------------------
  - id: CORE_TAGS
    title: "Core Tag System (Global)"
    depends_on: [CORE_TYPES]
    summary: >
      Profile-agnostic tag detection and normalization applied globally across the ruleset.
      Captures tags from headers, prefixes, and inline text for ANY dataset, then maps to
      normalized fields. Non-destructive; raw columns preserved.
    config:
      detection:
        headers_any_of:
          - "Tag"
          - "Tags"
          - "Labels"
          - "Label"
          - "Category"
          - "Categories"
          - "Valence"
          - "Channel"
          - "Module"
          - "Subtype"
          - "Owner"
          - "Person"
          - "Pair"
          - "Chart"
          - "Chart_Type"
          - "Profile"
          - "Context"
          - "Notes"
          - "sf_tags"
          - "sf_labels"
          - "meta_tags"
          - "meta_valence"
          - "meta_channel"
          - "meta_module"
        header_prefixes:
          - "tag_"
          - "tags_"
          - "meta_"
          - "label_"
          - "flag_"
          - "bool_"
        parse:
          list_delimiters: [",",";","|","/"]
          kv_delimiters: [":","="]
          strip_chars: [" ", "\t", "\n", "\"", "'"]
          lower_case: true
          dedupe: true
          drop_empty: true
          truthy: ["1","true","yes","y","on"]
          falsy:  ["0","false","no","n","off"]
        map_to_normalized:
          valence: ["valence","tone","affect"]
          channel: ["channel","chan","stream"]
          module:  ["module","mod","submodule","subtype"]
          owner:   ["owner","person","subject"]
          pair:    ["pair","pair_key","dyad"]
          chart:   ["chart","chart_type","profile"]
        output_fields:
          tags_array: "tags"
          tags_kv_json: "tags_kv"
          valence: "valence"
          channel: "channel"
          module:  "module"
          owner:   "person_key"
          pair:    "pair_key"
          chart:   "chart_profile"

    api:
      apply_to_any_table:
        steps:
          - collect_tags_from_headers:
              use: config.detection
          - collect_tags_from_free_text:
              fields_any_of: ["Notes","Comment","Comments","Context","Description","raw_line"]
              patterns:
                - r"(?:^|[\s,;|])#(?P<tag>[A-Za-z0-9_:\-=]+)"
                - r"(?:^|[\s,;|])\[(?P<tag>[A-Za-z0-9_:\-=]+)\]"
              to: "tags"
          - normalize_tag_keys_to_columns:
              from: "tags_kv"
              map: config.detection.map_to_normalized
              outputs: config.detection.output_fields
          - ensure_arrays_and_json:
              arrays: ["tags"]
              objects: ["tags_kv"]

    outputs:
      normalized_columns_add:
        - tags
        - tags_kv
        - valence
        - channel
        - module
        - person_key
        - pair_key
        - chart_profile

  # -------------------------------
  # SOLAR FIRE I/O (ALL TYPES) + SLICING
  # -------------------------------
  - id: SOLARFIRE_IO
    title: "Solar Fire Data I/O (All Types) + Range Slicing"
    depends_on: [MANIFEST, CORE_TYPES]
    summary: >
      Ingests Solar Fire-origin CSV/TXT/XLS-derived CSV for ephemeris, natal, composite,
      synastry/aspects, transits/progressions/directions logs, harmonics, draconic, house cusps, etc.
      Retains all raw fields (lossless), adds a normalized schema, and enables time-window slicing
      for datasets with datetime columns. Routed by dataset_profiles with auto-detection & manifest hints.
    config:
      timezone_default: "Etc/UTC"
      slice_padding:
        hours: 12
        days: 2
      chunking:
        rows_per_chunk: 200000
      index:
        enable: true
        type: "row_seek"
        filename_suffix: ".sfi.index.csv"
      retain_all_raw_columns: true
    io:
      detect_format:
        matchers:
          - when: file_extension in ["csv"]     ; assume: "csv"
          - when: file_extension in ["txt"]     ; assume: "txt"
          - when: file_extension in ["xls","xlsx"] ; assume: "xls_csv"
      dataset_profiles:
        ephem_hourly:
          match_any_header: ["Date","Time","UT","GMT","Lon","Longitude","Body","Planet"]
          dt_fields: {date: ["Date"], time: ["Time","UT","GMT"], ampm: []}
          index_granularity: "hour"
          kind: "ephemeris"
        ephem_qtrmonth:
          match_any_header: ["Date","Lon","Longitude","Body","Planet","Step","Incr"]
          dt_fields: {date: ["Date"], time: [], ampm: []}
          infer_time: "00:00"
          index_granularity: "day"
          kind: "ephemeris"
        transits_log:
          match_any_header: ["Date","Time","Transit","Aspect","To","Orb"]
          dt_fields: {date: ["Date"], time: ["Time"], ampm: []}
          index_granularity: "minute"
          kind: "aspects_transit"
        progressions_log:
          match_any_header: ["Date","Time","Progressed","Aspect","To","Orb"]
          dt_fields: {date: ["Date"], time: ["Time"], ampm: []}
          index_granularity: "minute"
          kind: "aspects_progressed"
        directions_log:
          match_any_header: ["Date","Time","Directed","Aspect","To","Orb"]
          dt_fields: {date: ["Date"], time: ["Time"], ampm: []}
          index_granularity: "minute"
          kind: "aspects_directed"
        natal_chart:
          match_any_header: ["Body","Planet","Longitude","Lat","Speed","House","Decl"]
          static_table: true
          kind: "natal"
        composite_midpoint:
          match_any_header: ["Body","Longitude","House","Speed","Decl","RA"]
          static_table: true
          kind: "composite_midpoint"
        composite_davison:
          match_any_header: ["Body","Longitude","House","Decl","RA"]
          static_table: true
          kind: "composite_davison"
        synastry_matrix:
          match_any_header: ["Body_A","Body_B","Aspect","Orb","Score","Angle"]
          static_table: true
          kind: "synastry"
        harmonics:
          match_any_header: ["H","Body","Longitude","House","Speed"]
          static_table: true
          kind: "harmonics"
        draconic_natal:
          match_any_header: ["Body","Longitude","House","Draconic"]
          static_table: true
          kind: "draconic"
        house_cusps:
          match_any_header: ["House","Longitude","Sign","Degree"]
          static_table: true
          kind: "houses"

      normalization:
        steps:
          - coerce_datetime_if_present:
              from_fields: {date: ["Date"], time: ["Time","UT","GMT"], ampm: []}
              infer_time_if_missing: "00:00"
              to_local: dt_local
              tz_assume: config.timezone_default
              to_utc: dt_utc
          - detect_body:
              fields_any_of: ["Body","Planet","Transit","Progressed","Directed"]
              to: body
          - detect_target:
              fields_any_of: ["To","Aspect_Target","Target","Point"]
              to: aspect_target
          - detect_aspect:
              fields_any_of: ["Aspect","Asp","Type"]
              to: aspect_type
          - longitude_decimal:
              fields_any_of: ["Longitude","Lon","Lon°","Ecliptic Lon"]
              to: lon_deg
          - latitude_decimal:
              fields_any_of: ["Latitude","Lat","Lat°"]
              to: lat_deg
          - speed_decimal:
              fields_any_of: ["Speed","Spd","Speed°/day"]
              to: speed_deg_per_day
          - compute_sign_from_longitude:
              src: lon_deg
              to: sign
          - derive_retrograde:
              from: [speed_deg_per_day, "R","Retro"]
              to: retrograde
          - passthrough_raw:
              prefix: "raw_"
              include_unmapped: true
          - stamp_source_metadata:
              fields:
                source_file: "{{input.filepath}}"
                source_format: "{{io.detect_format}}"
                dataset_kind: "{{io.dataset_profiles.kind}}"
                scale_tag: "{{manifest.scale_tag}}"

      validation:
        rules:
          - if: dataset_kind in ["ephemeris","aspects_transit","aspects_progressed","aspects_directed"]
            require_any: [["dt_utc","dt_local"]]
          - if: dataset_kind in ["natal","composite_midpoint","composite_davison","synastry","harmonics","draconic","houses"]
            require_any: [["body","lon_deg"]]
        warn_on_missing: ["speed_deg_per_day","julian_day","house"]
        strict: false

    slicing:
      select:
        inputs:
          - dataset: one_of[auto, ephemeris, transits, progressions, directions, natal, composite, synastry, harmonics, draconic, houses]
          - window_start: optional datetime
          - window_end:   optional datetime
          - scale: optional one_of[auto, ephem_hourly, ephem_qtrmonth]
          - person_key: optional
          - pair_key: optional
          - tz: optional
        algorithm: |
          1) Resolve MANIFEST.lookup(dataset, scale, window) -> {path(s), scale_tag, dataset_kind}.
          2) If dataset_kind is time-based:
                a) Ensure/build sidecar index with configured granularity.
                b) Expand window by slice_padding; find intersecting blocks.
                c) Stream-load only those row ranges; parse/normalize on the fly.
             Else (static tables):
                a) Load with projection/filtering (person_key, pair_key) if provided.
          3) Yield normalized rows with all raw_* columns intact.

      index_builder:
        key_granularity_by_kind:
          ephemeris: "hour"
          aspects_transit: "minute"
          aspects_progressed: "minute"
          aspects_directed: "minute"
        output_columns: [dt_block_start_utc, dt_block_end_utc, start_row, end_row]

    outputs:
      normalized_columns:
        - dt_utc
        - dt_local
        - julian_day
        - body
        - lon_deg
        - lat_deg
        - speed_deg_per_day
        - sign
        - house
        - retrograde
        - aspect_type
        - aspect_target
        - orb_deg
        - dataset_kind
        - source_file
        - source_row
        - source_format
        - scale_tag

  # -------------------------------
  # EXTERNAL SOURCE I/O (WEB/PASTE)
  # -------------------------------
  - id: EXTERNAL_SOURCE_IO
    title: "External Astrology Sources I/O (web & paste)"
    depends_on: [CORE_TYPES, MANIFEST, CORE_TAGS]
    summary: >
      Ingest and normalize astrology data coming from third-party sites (e.g., Astro-Seek, Astro.com),
      exported CSV/HTML tables, or user copy/paste text. Preserves all raw fields; adds a normalized layer
      compatible with the ruleset; supports date/time slicing when timestamps exist; records provenance.
    config:
      timezone_default: "Etc/UTC"
      slice_padding:
        hours: 12
        days: 2
      index:
        enable: true
        type: "row_seek"
        filename_suffix: ".ext.index.csv"
      parsing:
        delimiter_candidates: [",",";","\t","|"]
        decimal_separators: [".", ","]
        header_aliases:
          date: ["Date","DATE","Dátum","Datum","Fecha"]
          time: ["Time","TIME","Čas","Uhrzeit","Hora","UT","GMT"]
          body: ["Body","Planet","Point","Obj","Objekt"]
          lon:  ["Longitude","Lon","λ","Ecliptic Lon","Long.","Lon°"]
          lat:  ["Latitude","Lat","β","Lat°"]
          speed: ["Speed","v","Speed°/day","Daily motion"]
          aspect: ["Aspect","Asp","Type","Angle"]
          target: ["To","Target","Point2","With","Aspect_To"]
          orb: ["Orb","Orb°","Δ"]
          house: ["House","Hs","H"]
          sign: ["Sign","Zodiac","Constellation"]
      html_table:
        recognize: ["<table", "</table>", "<tr", "<td", "<th"]
      text_regex:
        patterns:
          - name: "date_time_body_lon_sign_speed"
            regex: >
              ^\s*(?P<date>\d{1,2}[\-\/\s]\w{3}[\-\/\s]\d{2,4}|\d{4}[\-\/]\d{2}[\-\/]\d{2}|\d{1,2}[\-\/]\d{1,2}[\-\/]\d{2,4})
              (?:\s+(?P<time>\d{1,2}:\d{2}(?::\d{2})?\s*(?:AM|PM)?))?\s+
              (?P<body>[A-Za-z\p{L}][A-Za-z\p{L}\-_\.]*)\s+
              (?P<lon_dms>\d{1,3}[°\s]\d{1,2}[′'\s]\d{1,2}(?:[″"])?)
              \s*(?P<sign>[A-Za-z]{3,9})\s+
              (?P<speed>[-+]?\d+(?:[\.,]\d+)?)\s*(?P<retro>R)?\s*$
          - name: "body_lon_sign_only"
            regex: >
              ^\s*(?P<body>[A-Za-z\p{L}][A-Za-z\p{L}\-_\.]*)\s+
              (?P<lon_dms>\d{1,3}[°\s]\d{1,2}[′'\s]\d{1,2}(?:[″"])?)
              \s*(?P<sign>[A-Za-z]{3,9})\s*$
      dms_to_decimal:
        sign_order: ["Aries","Taurus","Gemini","Cancer","Leo","Virgo","Libra","Scorpio","Sagittarius","Capricorn","Aquarius","Pisces"]
        sign_to_start_deg:
          Aries: 0
          Taurus: 30
          Gemini: 60
          Cancer: 90
          Leo: 120
          Virgo: 150
          Libra: 180
          Scorpio: 210
          Sagittarius: 240
          Capricorn: 270
          Aquarius: 300
          Pisces: 330

    io:
      adapters:
        - name: "astro-seek-csv"
          detect:
            any_url_contains: ["astro-seek.com"]
            any_header_contains: ["Astro-Seek","AS_EXPORT","Object"]
          parse: "csv"
        - name: "generic-csv"
          detect:
            file_extension_in: ["csv","tsv"]
          parse: "csv_autodelim"
        - name: "html-table"
          detect:
            snippet_contains_any: config.html_table.recognize
          parse: "html_table"
        - name: "pasted-text"
          detect:
            always: true
          parse: "text_regex"

      parsers:
        csv:
          delimiter: ","
          quoting: auto
          encoding: "utf-8-sig"
        csv_autodelim:
          delimiters: config.parsing.delimiter_candidates
          decimal_separators: config.parsing.decimal_separators
          encoding: "utf-8-sig"
        html_table:
          strip_tags: true
          header_row_guess: first
        text_regex:
          patterns: text_regex.patterns
          keep_unmatched_as_raw_line: true

      normalization:
        steps:
          - canonicalize_headers:
              alias_map: config.parsing.header_aliases
          - coerce_datetime_if_present:
              from_fields: {date: ["date"], time: ["time","ut","gmt"], ampm: []}
              infer_time_if_missing: "00:00"
              to_local: dt_local
              tz_assume: config.timezone_default
              to_utc: dt_utc
          - dms_or_sign_to_longitude:
              from: ["lon","longitude","lon_dms","long."]
              sign_field_candidates: ["sign"]
              dms_parser: dms_to_decimal
              to: lon_deg
          - latitude_decimal_if_present:
              from: ["lat","latitude","β","lat°","lat_dms"]
              to: lat_deg
          - speed_decimal_if_present:
              from: ["speed","v","speed°/day","daily motion"]
              decimal_separators: config.parsing.decimal_separators
              to: speed_deg_per_day
          - compute_sign_from_longitude_if_missing:
              src: lon_deg
              to: sign
          - derive_retrograde_if_present:
              from: [speed_deg_per_day,"retro","r"]
              to: retrograde
          - detect_body_if_missing:
              fields_any_of: ["body","planet","object","point","obj","transit","progressed","directed"]
              to: body
          - detect_aspect_and_target_if_present:
              aspect_fields_any_of: ["aspect","type","angle"]
              target_fields_any_of: ["to","target","point2","with","aspect_to"]
              to_aspect: aspect_type
              to_target: aspect_target
          - passthrough_raw:
              prefix: "raw_"
              include_unmapped: true
          - provenance_stamp:
              fields:
                source_kind: "{{io.adapters.name}}"
                source_url: "{{input.source_url}}"
                source_label: "{{input.source_label}}"
                source_file: "{{input.filepath}}"

      validation:
        rules:
          - if: any(["dt_utc","dt_local","lon_deg","body"])
            require_any: [["lon_deg","body"],["dt_utc","body"]]
        warn_on_missing: ["speed_deg_per_day","house","orb_deg"]
        strict: false

    slicing:
      select:
        inputs:
          - window_start: optional datetime
          - window_end: optional datetime
          - tz: optional
        algorithm: |
          If dt columns exist: ensure/build sidecar index (.ext.index.csv) at hour granularity.
          Expand requested window by slice_padding; stream only intersecting blocks.
          If no dt columns: load full table; rely on downstream filters (non-time datasets).

    outputs:
      normalized_columns_add:
        - dt_utc
        - dt_local
        - body
        - lon_deg
        - lat_deg
        - speed_deg_per_day
        - sign
        - retrograde
        - aspect_type
        - aspect_target
        - orb_deg
        - source_kind
        - source_url
        - source_label
        - source_file

  # -------------------------------
  # GENERIC FLEXIBLE I/O (CATCH-ALL)
  # -------------------------------
  - id: GENERIC_FLEX_IO
    title: "Generic Flexible I/O (schema-from-context & tags)"
    depends_on: [CORE_TYPES, CORE_TAGS]
    summary: >
      Catch-all ingest for unexpected data. Autodetects basic structures (CSV/TSV, JSON/JSONL, YAML,
      Markdown tables, key:value text, freeform logs), then infers a working schema based on content,
      tags, and lightweight heuristics. Preserves raw data; emits normalized columns plus a 'payload'
      JSON for anything non-tabular. Supports time slicing if datetime fields are discovered.
    config:
      timezone_default: "Etc/UTC"
      slice_padding:
        hours: 4
        days: 1
      index:
        enable: true
        filename_suffix: ".gen.index.csv"
      autodetect:
        prefer:
          - "jsonl"
          - "csv/tsv"
          - "markdown_table"
          - "yaml"
          - "kv_text"
          - "free_text"
        datetime_headers_like: ["date","time","timestamp","ts","dt","datetime","utc","gmt"]
        longitude_headers_like: ["lon","longitude","ecliptic lon","λ"]
        latitude_headers_like: ["lat","latitude","β"]
        body_headers_like: ["body","planet","point","object","name","id"]
      json:
        line_delimited: true
        coerce_scalars: true
      kv_text:
        separators: [":","=","→","->"]
        pair_delims: [",",";","|"]
      markdown_table:
        header_sep_match: r"^\s*\|?(?:\s*:?-+:?\s*\|)+\s*$"

    io:
      adapters:
        - name: "jsonl"
          detect:
            snippet_starts_with: ["{","["]
            contains: ["}\n{","}\r\n{"]
          parse: "json_lines"
        - name: "csv_tsv"
          detect:
            regex_any: [",\s*\w+", "\t\w+"]
          parse: "csv_autodelim"
        - name: "markdown_table"
          detect:
            regex_any: ["^\s*\|.*\|\s*$", config.markdown_table.header_sep_match]
          parse: "md_table"
        - name: "yaml"
          detect:
            snippet_starts_with: ["---","\n- ","\nkey: "]
          parse: "yaml"
        - name: "kv_text"
          detect:
            regex_any: ["^\s*\w+\s*[:=]\s*.+$"]
          parse: "kv_text"
        - name: "free_text"
          detect:
            always: true
          parse: "free_text"

      parsers:
        json_lines:
          line_delimited: config.json.line_delimited
        csv_autodelim:
          delimiters: [",",";","\t","|"]
          encoding: "utf-8-sig"
        md_table:
          # simple markdown table parser
          strip_pipes: true
        yaml: {}
        kv_text:
          separators: config.kv_text.separators
          pair_delims: config.kv_text.pair_delims
        free_text:
          keep_verbatim: true

      normalization:
        steps:
          - infer_headers_and_types:
              prefer: config.autodetect.prefer
              datetime_like: config.autodetect.datetime_headers_like
          - coerce_datetime_if_present:
              from_fields: {date: ["date","timestamp","ts","dt","datetime","utc","gmt"], time: ["time"], ampm: []}
              infer_time_if_missing: "00:00"
              to_local: dt_local
              tz_assume: config.timezone_default
              to_utc: dt_utc
          - map_likely_longitude:
              fields_like: config.autodetect.longitude_headers_like
              to: lon_deg
          - map_likely_latitude:
              fields_like: config.autodetect.latitude_headers_like
              to: lat_deg
          - map_likely_body:
              fields_like: config.autodetect.body_headers_like
              to: body
          - emit_payload_for_non_tabular:
              to: payload_json
          - passthrough_raw:
              prefix: "raw_"
              include_unmapped: true

      validation:
        rules:
          - if: any(["dt_utc","lon_deg","lat_deg","body","payload_json"])
            require_any: [["payload_json"],["body"],["lon_deg"],["dt_utc"]]
        strict: false

    slicing:
      select:
        inputs:
          - window_start: optional datetime
          - window_end: optional datetime
        algorithm: |
          If datetime discovered: build/use .gen.index.csv (hour granularity); pad window;
          stream overlapping slices only. Otherwise, load fully (assumed small or non-time).

    outputs:
      normalized_columns_add:
        - dt_utc
        - dt_local
        - body
        - lon_deg
        - lat_deg
        - payload_json

global_hooks:
  normalization_post:
    apply_to: ["*"]
    call:
      module: CORE_TAGS.apply_to_any_table
      with: {}

# -------------------------------
# MANIFEST ENTRIES (datasets + external + ephemeris map)
# -------------------------------
manifest:
  datasets:
    - id: ephem_hourly_2025
      dataset: ephemeris
      scale: ephem_hourly
      path: "/mnt/data/2025 Ephemeris - 1HR.xls.csv"
      date_range: { start: "2025-01-01T00:00:00Z", end: "2025-12-31T23:59:59Z" }
      scale_tag: "ephem_hourly_2025"

    - id: ephem_qtrmonth_1900_2050
      dataset: ephemeris
      scale: ephem_qtrmonth
      path: "/mnt/data/Ephemeris 1900-2050 0.25 month steps.csv"
      date_range: { start: "1900-01-01T00:00:00Z", end: "2050-12-31T23:59:59Z" }
      scale_tag: "ephem_qtrmonth_1900_2050"

    - id: transits_log_merged
      dataset: transits
      scale: auto
      path: "/mnt/data/Chris_ALL_MODULES_MERGED.csv"
      hints: { column_keys: ["Date","Time","Transit","Aspect","To"] }

    - id: progressions_log_merged
      dataset: progressions
      scale: auto
      path: "/mnt/data/Chris_ALL_MODULES_MERGED.csv"
      hints: { column_keys: ["Progressed","Aspect","To"] }

    - id: directions_log_merged
      dataset: directions
      scale: auto
      path: "/mnt/data/Chris_ALL_MODULES_MERGED.csv"
      hints: { column_keys: ["Directed","Aspect","To"] }

    - id: natal_cv
      dataset: natal
      path: "/mnt/data/CV_Natal_Chart.csv"
      person_key: "Chris"

    - id: natal_remy
      dataset: natal
      path: "/mnt/data/Remy_ALL_MODULES_MERGED.csv"
      person_key: "Remy"

    - id: composite_midpoints_cv
      dataset: composite
      subtype: composite_midpoint
      path: "/mnt/data/CV_CR_Composite_Midpoints.csv"
      pair_key: "Chris&Remy"

    - id: composite_all_cr
      dataset: composite
      subtype: auto
      path: "/mnt/data/Chris_&_Remy_-_Composite_ALL_MERGED.csv"
      pair_key: "Chris&Remy"

    - id: harmonics_cr
      dataset: harmonics
      path: "/mnt/data/Chris_Remy_Harmonics_1-12_with_numbers.csv"
      pair_key: "Chris&Remy"

    - id: draconic_cr
      dataset: draconic
      path: "/mnt/data/Chris_Remy_Draconic_Natal_Structured.csv"
      pair_key: "Chris&Remy"

  ephemeris_map:
    id: ephem_auto_map
    dataset: ephemeris
    scale: auto
    path: "/mnt/data/Ephemeris_MERGED.csv"
    subscales:
      - { use: ephem_hourly_2025,       start: "2025-01-01", end: "2025-12-31" }
      - { use: ephem_qtrmonth_1900_2050, start: "1900-01-01", end: "2050-12-31" }

  external_sources:
    - id: astro_seek_generic
      site: "astro-seek.com"
      patterns: ["https://www.astro-seek.com/*"]
      notes: "Generic adapter routes via csv/html/paste detection."
    - id: astro_com_generic
      site: "astro.com"
      patterns: ["https://www.astro.com/*"]
      notes: "Generic adapter; expects CSV export or pasted table."
    - id: paste_buffer
      site: "paste"
      patterns: ["PASTE_BUFFER"]
      notes: "When user pastes text blocks; EXTERNAL_SOURCE_IO detects and parses."
